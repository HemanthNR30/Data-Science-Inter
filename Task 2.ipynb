{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhO8m4wxro62Q1vvK+C9uw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6nwhXF8Y5Sp","executionInfo":{"status":"ok","timestamp":1751984148167,"user_tz":-330,"elapsed":19777,"user":{"displayName":"Hemanth Bharadwaj NR","userId":"15328730901172521359"}},"outputId":"acf61ab7-a964-4acd-9a77-e04b02b2cae8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9605381165919282\n","Confusion Matrix:\n"," [[965   1]\n"," [ 43 106]]\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98       966\n","           1       0.99      0.71      0.83       149\n","\n","    accuracy                           0.96      1115\n","   macro avg       0.97      0.86      0.90      1115\n","weighted avg       0.96      0.96      0.96      1115\n","\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","df = pd.read_csv('https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv', sep='\\t', names=['label', 'message'])\n","\n","df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    text = re.sub(r'\\W', ' ', text)\n","    text = text.lower()\n","    text = text.split()\n","    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n","    return ' '.join(text)\n","\n","df['clean_text'] = df['message'].apply(preprocess)\n","\n","X = df['clean_text']\n","y = df['label']\n","\n","vectorizer = TfidfVectorizer()\n","X_vec = vectorizer.fit_transform(X)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n","\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"]}]}